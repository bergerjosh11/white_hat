import numpy as np
import tensorflow as tf
from AI_model import AIModel
from bs4 import BeautifulSoup
import requests
import time
from robotexclusionrulesparser import RobotExclusionRulesParser

# Instantiate your AIModel
ai_model = AIModel("AI_model.h5")  # Update the path accordingly

# Initialize URL queue and visited set
url_queue = ["https://www.google.com"]  # Starting URL
visited_urls = set()

# Initialize lists to store features (X) and labels (y)
X = []
y = []

# Maximum number of pages to crawl (adjust as needed)
max_pages = 10

# Initialize RobotExclusionRulesParser
robot_parser = RobotExclusionRulesParser()

# Define function to fetch robots.txt content
def fetch_robots_txt(url):
    try:
        robots_txt_url = f"{url.rstrip('/')}/robots.txt"
        response = requests.get(robots_txt_url)
        if response.status_code == 200:
            return response.text
        return None
    except requests.exceptions.RequestException as e:
        return None

# Define function to check if URL is allowed by robots.txt
def is_allowed_by_robots_txt(url):
    robots_txt = fetch_robots_txt(url)
    if robots_txt:
        robot_parser.parse(robots_txt)
        return robot_parser.can_fetch("*", url)
    return True

# Perform web crawl
while url_queue and len(visited_urls) < max_pages:
    current_url = url_queue.pop(0)  # Get the next URL from the queue

    if not is_allowed_by_robots_txt(current_url):
        continue

    # Check if URL has been visited
    if current_url in visited_urls:
        continue

    # Visit the URL and extract features
    try:
        response = requests.get(current_url)
        if response.status_code == 200:
            content = response.content
            soup = BeautifulSoup(content, 'html.parser')
            features = ai_model.extract_features(soup)  # Use AIModel's extract_features method

            # Process features and update your dataset
            if features is not None:
                X.append(features)  # Add extracted features to the X list
                y.append(1)  # Add corresponding label (indicating vulnerability) to the y list

                visited_urls.add(current_url)  # Mark URL as visited

                # Extract links from the page and add to the queue
                links = [link.get("href") for link in soup.find_all("a")]
                for link in links:
                    if link and link.startswith("http"):
                        url_queue.append(link)

                time.sleep(1)  # Add a delay to be polite to the server
        else:
            print("Failed to fetch URL:", current_url, "Status Code:", response.status_code)
    except requests.exceptions.RequestException as e:
        print("Error:", e)

# Convert lists to numpy arrays
X = np.array(X)
y = np.array(y)

# Train a new model using the collected dataset
new_model = tf.keras.Sequential([
    # Define your model layers here based on your architecture
])
new_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
new_model.fit(X, y, epochs=10)  # Adjust epochs as needed

# Save the trained model
new_model.save("trained_model.h5")
